{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding, GRU, Input, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    print text\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    text = \" \".join([w for w in twtk.tokenize(text) if w != \"\" and w is not None])\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for cleaning\n",
    "def removeStopwords(tokens):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops.update(['.',',','\"',\"'\",'?',':',';','(',')','[',']','{','}'])\n",
    "    toks = [tok for tok in tokens if not tok in stops and len(tok) >= 3]\n",
    "    return toks\n",
    "\n",
    "def removeURL(text):\n",
    "    newText = re.sub('http\\\\S+', '', text, flags=re.MULTILINE)\n",
    "    return newText\n",
    "\n",
    "def removeNum(text):\n",
    "    newText = re.sub('\\\\d+', '', text)\n",
    "    return newText\n",
    "\n",
    "def removeHashtags(tokens):\n",
    "    toks = [ tok for tok in tokens if tok[0] != '#']\n",
    "#     if segment == True:\n",
    "#         segTool = Analyzer('en')\n",
    "#         for i, tag in enumerate(self.hashtags):\n",
    "#             text = tag.lstrip('#')\n",
    "#             segmented = segTool.segment(text)\n",
    "\n",
    "    return toks\n",
    "\n",
    "def stemTweet(tokens):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet(tweet, remove_swords = True, remove_url = True, remove_hashtags = True, remove_num = True, stem_tweet = True):\n",
    "#     text = tweet.translate(string.punctuation)   -> to figure out what it does ?\n",
    "    \"\"\"\n",
    "        Tokenize the tweet text using TweetTokenizer.\n",
    "        set strip_handles = True to Twitter username handles.\n",
    "        set reduce_len = True to replace repeated character sequences of length 3 or greater with sequences of length 3.\n",
    "    \"\"\"\n",
    "    if remove_url:\n",
    "        tweet = removeURL(tweet)\n",
    "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tokens = [w.lower() for w in twtk.tokenize(tweet) if w != \"\" and w is not None]\n",
    "    if remove_hashtags:\n",
    "        tokens = removeHashtags(tokens)\n",
    "    if remove_swords:\n",
    "        tokens = removeStopwords(tokens)\n",
    "    if stem_tweet:\n",
    "        tokens = stemTweet(tokens)\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Hurray, saving us $$$ in so many ways @potus @realDonaldTrump #LockThemUp #BuildTheWall #EndDACA #BoycottNFL #BoycottNike\n",
      "\n",
      "hurray save mani way\n",
      "\n",
      "1 Why would young fighting age men be the vast majority of the ones escaping a war &amp; not those who cannot fight like women, children, and the elderly?It's because the majority of the refugees are not actually refugees they are economic migrants trying to get into Europe.... https://t.co/Ks0SHbtYqn\n",
      "\n",
      "would young fight age men vast major one escap war cannot fight like women children elder major refuge actual refuge econom migrant tri get europ ...\n",
      "\n",
      "2 @KamalaHarris Illegals Dump their Kids at the border like Road Kill and Refuse to Unite! They Hope they get Amnesty, Free Education and Welfare Illegal #FamilesBelongTogether in their Country not on the Taxpayer Dime Its a SCAM #NoDACA #NoAmnesty #SendThe\n",
      "\n",
      "illeg dump kid border like road kill refus unit hope get amnesti free educ welfar illeg countri taxpay dime scam\n",
      "\n",
      "3 NY Times: 'Nearly All White' States Pose 'an Array of Problems' for Immigrants https://t.co/ACZKLhdMV9 https://t.co/CJAlSXCzR6\n",
      "\n",
      "time near white state pose array problem immigr\n",
      "\n",
      "4 Orban in Brussels: European leaders are ignoring the will of the people, they do not want migrants https://t.co/NeYFyqvYlX\n",
      "\n",
      "orban brussel european leader ignor peopl want migrant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('./public_development_en/train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "cleaned = train_data['text'][:5].map(lambda x: processTweet(x))\n",
    "for i,t in enumerate(cleaned):\n",
    "    print i,train_data['text'][i]\n",
    "    print \n",
    "    print t\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/aa_IRE_project/ireenv/lib/python2.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(10687, 128, embeddings_regularizer=<keras.reg..., input_length=140)`\n",
      "/home/ash/aa_IRE_project/ireenv/lib/python2.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(128, recurrent_regularizer=<keras.reg..., bias_regularizer=<keras.reg..., kernel_regularizer=<keras.reg..., return_sequences=False)`\n",
      "/home/ash/aa_IRE_project/ireenv/lib/python2.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, activation=\"softmax\", kernel_regularizer=<keras.reg...)`\n",
      "/home/ash/aa_IRE_project/ireenv/lib/python2.7/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/10\n",
      "8100/8100 [==============================] - 72s 9ms/step - loss: 0.7506 - acc: 0.7051 - val_loss: 1.1573 - val_acc: 0.5078\n",
      "Epoch 2/10\n",
      "8100/8100 [==============================] - 73s 9ms/step - loss: 0.5608 - acc: 0.7633 - val_loss: 1.0235 - val_acc: 0.5511\n",
      "Epoch 3/10\n",
      "8100/8100 [==============================] - 74s 9ms/step - loss: 0.5422 - acc: 0.7693 - val_loss: 0.8028 - val_acc: 0.6222\n",
      "Epoch 4/10\n",
      "8100/8100 [==============================] - 70s 9ms/step - loss: 0.5341 - acc: 0.7711 - val_loss: 0.9962 - val_acc: 0.5811\n",
      "Epoch 5/10\n",
      "8100/8100 [==============================] - 70s 9ms/step - loss: 0.5289 - acc: 0.7735 - val_loss: 0.8516 - val_acc: 0.6122\n",
      "Epoch 6/10\n",
      "8100/8100 [==============================] - 75s 9ms/step - loss: 0.5241 - acc: 0.7775 - val_loss: 0.8570 - val_acc: 0.6300\n",
      "Epoch 7/10\n",
      "8100/8100 [==============================] - 77s 9ms/step - loss: 0.5166 - acc: 0.7789 - val_loss: 0.9479 - val_acc: 0.5444\n",
      "Epoch 8/10\n",
      "4448/8100 [===============>..............] - ETA: 30s - loss: 0.4998 - acc: 0.7934"
     ]
    }
   ],
   "source": [
    "# tweets = train_data['text']\n",
    "maxlen = 140\n",
    "train_data['text'] = train_data['text'].map(lambda x: processTweet(x))\n",
    "vocabulary_size = 30000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
    "# print(sequences)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "labels = train_data['HS']\n",
    "Y_train = np_utils.to_categorical(labels, len(set(labels)))\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "l2_coef = 0.001\n",
    "tweet = Input(shape=(maxlen,), dtype='int32')\n",
    "x = Embedding(V, 128, input_length=maxlen, W_regularizer=l2(l=l2_coef))(tweet)\n",
    "x = Bidirectional(layer=GRU(128, return_sequences=False, \n",
    "                            W_regularizer=l2(l=l2_coef),\n",
    "                            b_regularizer=l2(l=l2_coef),\n",
    "                            U_regularizer=l2(l=l2_coef)),\n",
    "                  merge_mode='sum')(x)\n",
    "x = Dense(len(set(labels)), W_regularizer=l2(l=l2_coef), activation=\"softmax\")(x)\n",
    "\n",
    "tweet2vec = Model(input=tweet, output=x)\n",
    "\n",
    "tweet2vec.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tweet2vec.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "embeddings_index = dict()\n",
    "f = open('./glove.twitter.27B/glove.twitter.27B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "print(tokenizer.word_index.items())\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocabulary_size, 200))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 200, input_length=50, weights=[embedding_matrix], trainable=True))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(200))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model_glove.fit(data, np.array(labels), validation_split=0.3, epochs = 20)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
