{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding, GRU, Input, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    text = \" \".join([w for w in twtk.tokenize(text) if w != \"\" and w is not None])\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'id', u'text', u'HS', u'TR', u'AG']\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('./public_development_en/train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "print(list(train_data.columns.values)) #file header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/aa_IRE_project/ireenv/lib/python2.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(16881, 128, embeddings_regularizer=<keras.reg..., input_length=140)`\n",
      "/home/ash/aa_IRE_project/ireenv/lib/python2.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(128, recurrent_regularizer=<keras.reg..., bias_regularizer=<keras.reg..., kernel_regularizer=<keras.reg..., return_sequences=False)`\n",
      "/home/ash/aa_IRE_project/ireenv/lib/python2.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, activation=\"softmax\", kernel_regularizer=<keras.reg...)`\n",
      "/home/ash/aa_IRE_project/ireenv/lib/python2.7/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/10\n",
      "8100/8100 [==============================] - 105s 13ms/step - loss: 0.7152 - acc: 0.7606 - val_loss: 2.0931 - val_acc: 0.3133\n",
      "Epoch 2/10\n",
      "8100/8100 [==============================] - 94s 12ms/step - loss: 0.5029 - acc: 0.8022 - val_loss: 1.0783 - val_acc: 0.5722\n",
      "Epoch 3/10\n",
      "8100/8100 [==============================] - 71s 9ms/step - loss: 0.4830 - acc: 0.8067 - val_loss: 0.9992 - val_acc: 0.6144\n",
      "Epoch 4/10\n",
      "8100/8100 [==============================] - 71s 9ms/step - loss: 0.4707 - acc: 0.8131 - val_loss: 1.0198 - val_acc: 0.5500\n",
      "Epoch 5/10\n",
      "8100/8100 [==============================] - 71s 9ms/step - loss: 0.4615 - acc: 0.8177 - val_loss: 0.9874 - val_acc: 0.6011\n",
      "Epoch 6/10\n",
      "8100/8100 [==============================] - 71s 9ms/step - loss: 0.4554 - acc: 0.8221 - val_loss: 0.9391 - val_acc: 0.5511\n",
      "Epoch 7/10\n",
      "8100/8100 [==============================] - 71s 9ms/step - loss: 0.4486 - acc: 0.8193 - val_loss: 1.0296 - val_acc: 0.5844\n",
      "Epoch 8/10\n",
      "8100/8100 [==============================] - 71s 9ms/step - loss: 0.4462 - acc: 0.8251 - val_loss: 0.9299 - val_acc: 0.5822\n",
      "Epoch 9/10\n",
      "8100/8100 [==============================] - 71s 9ms/step - loss: 0.4382 - acc: 0.8237 - val_loss: 1.3103 - val_acc: 0.4778\n",
      "Epoch 10/10\n",
      "8100/8100 [==============================] - 71s 9ms/step - loss: 0.4343 - acc: 0.8272 - val_loss: 0.9799 - val_acc: 0.5911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b10869d90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets = train_data['text']\n",
    "maxlen = 140\n",
    "train_data['text'] = train_data['text'].map(lambda x: clean_text(x))\n",
    "vocabulary_size = 30000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
    "# print(sequences)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "labels = train_data['HS']\n",
    "Y_train = np_utils.to_categorical(labels, len(set(labels)))\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "l2_coef = 0.001\n",
    "tweet = Input(shape=(maxlen,), dtype='int32')\n",
    "x = Embedding(V, 128, input_length=maxlen, W_regularizer=l2(l=l2_coef))(tweet)\n",
    "x = Bidirectional(layer=GRU(128, return_sequences=False, \n",
    "                            W_regularizer=l2(l=l2_coef),\n",
    "                            b_regularizer=l2(l=l2_coef),\n",
    "                            U_regularizer=l2(l=l2_coef)),\n",
    "                  merge_mode='sum')(x)\n",
    "x = Dense(len(set(labels)), W_regularizer=l2(l=l2_coef), activation=\"softmax\")(x)\n",
    "\n",
    "tweet2vec = Model(input=tweet, output=x)\n",
    "\n",
    "tweet2vec.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tweet2vec.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "embeddings_index = dict()\n",
    "f = open('./glove.twitter.27B/glove.twitter.27B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "print(tokenizer.word_index.items())\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocabulary_size, 200))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 200, input_length=50, weights=[embedding_matrix], trainable=True))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(200))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model_glove.fit(data, np.array(labels), validation_split=0.3, epochs = 20)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
